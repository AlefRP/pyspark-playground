{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark in Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing PySpark is not as straightforward as the usual process in Python. It involves more than just running a pip install. First, you need to install dependencies such as **Java 17** and **Apache Spark 3.5.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!apt-get install openjdk-17-jdk-headless -qq > /dev/null\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to set up the environment variables. This ensures that the Colab environment can correctly locate and use the installed dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
    "\n",
    "# Make PySpark importable\n",
    "import findspark\n",
    "findspark.init('/content/spark-3.5.1-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everything set up, let's run a local session to test if the installation worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Introduction\").getOrCreate()\n",
    "\n",
    "# Test the Spark session\n",
    "df = spark.createDataFrame([(1, 'foo'), (2, 'bar')], ['id', 'label'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 Using Spark with Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in using Spark is to connect to a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a practical scenario, the cluster will be hosted on a remote machine connected to all other nodes. This setup includes a primary machine known as the master, responsible for distributing data and tasks across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 Creating a SparkSession**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating multiple `SparkSessions` and `SparkContexts` can lead to issues, so it is a best practice to use the `SparkSession.builder.getOrCreate()` method. This method returns an existing `SparkSession` if it exists, otherwise, it creates a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a local session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Introduction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify SparkContext\n",
    "print(spark)\n",
    "\n",
    "# Print Spark version\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Using DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main data structure in Spark is the Resilient Distributed Dataset (RDD). This is a low-level object that enables Spark to perform its magic by distributing data across multiple nodes in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark DataFrame is designed to behave similarly to a SQL table (with variables in columns and observations in rows). Not only are DataFrames easier to understand, but they are also more optimized for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you start modifying and combining columns and rows of data, there are many ways to achieve the same result, but some methods are significantly more time-consuming than others. With RDDs, it's important to be aware of the performance implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin working with Spark DataFrames, you first need to create a `SparkSession` object from your `SparkContext`. You can think of the `SparkContext` as your connection to the cluster and the `SparkSession` as your interface for interacting with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize SparkSession with a specific application name\n",
    "spark = SparkSession.builder.appName(\"sp-functions\").getOrCreate()\n",
    "\n",
    "# Read CSV files from a specified directory, with headers and inferred schema\n",
    "df = spark.read.csv(r\"C:\\Users\\alefr\\PycharmProjects\\series-spark\\data\\*.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns using `select`\n",
    "df.select(\"cnpj_raiz\", \"socios\", \"atualizado_em\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns using `select` with column objects\n",
    "df.select(col(\"cnpj_raiz\"), col(\"socios\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant column using `lit`\n",
    "df.select(lit(1).alias(\"valid_row\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a calculated column using `withColumn` and a conditional expression\n",
    "df.withColumn(\"valid_row\", when(col(\"cnpj_raiz\").isNull(), lit(0)).otherwise(lit(1))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique IDs using `monotonically_increasing_id`\n",
    "df.select(monotonically_increasing_id().alias(\"id\"), \"cnpj_raiz\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with a schema\n",
    "df_dados = spark.createDataFrame([\n",
    "    (\"a\", 1, 3, 4),\n",
    "    (\"b\", 2, 4, 5),\n",
    "    (\"c\", 3, 5, 6)\n",
    "], schema='a string, b int, c int, d int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the greatest value among columns\n",
    "df_dados.select(\n",
    "    \"b\",\n",
    "    \"c\",\n",
    "    \"d\",\n",
    "    greatest(\"b\", \"c\", \"d\").alias(\"greatest_value\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQL-like expressions in `select`\n",
    "df.select(\"cnpj_raiz\", expr(\"CASE WHEN LENGTH(cnpj_raiz) = 8 THEN 1 ELSE 0 END\").alias(\"flag\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round numerical values to a specified number of decimal places\n",
    "df_dados.select(round(\"b\", 2).alias(\"b_rounded\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display current date and timestamp\n",
    "df.select(current_date(), current_timestamp()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to transform a DataFrame\n",
    "def make_upper(df):\n",
    "    return df.withColumn(\"a\", upper(col(\"a\")))\n",
    "\n",
    "# Apply the transformation function\n",
    "df_dados.transform(make_upper).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average of a column\n",
    "df_dados.select(avg(\"b\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on a condition\n",
    "df.filter(df[\"cnpj_raiz\"] == \"2421421\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by a column and count occurrences\n",
    "df_cnpj_por_data = df.groupBy(\"atualizado_em\").count()\n",
    "df_cnpj_por_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the grouped DataFrame as a Parquet table\n",
    "df_cnpj_por_data.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"cnpj_por_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
