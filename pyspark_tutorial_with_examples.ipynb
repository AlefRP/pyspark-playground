{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlefRP/Spark_PySpark/blob/main/pyspark_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6NB7jfwYVgl"
   },
   "source": [
    "#**PySpark in Google Colab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Yponzw2Ydzv"
   },
   "source": [
    "Installing PySpark is not as straightforward as the usual process in Python. It involves more than just running a pip install. First, you need to install dependencies such as **Java 17** and **Apache Spark 3.5.1** along with **Hadoop 3.3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3EI53MKB7ho7"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!apt-get install openjdk-17-jdk-headless -qq > /dev/null\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FWZTasE7jRD"
   },
   "source": [
    "The next step is to set up the environment variables. This ensures that the Colab environment can correctly locate and use the installed dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_cLWiFW8fSz"
   },
   "source": [
    "To interact with the terminal and manipulate it, you can use the **os** library in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nTkx7B9T8iuh"
   },
   "outputs": [],
   "source": [
    "# Configure environment variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
    "\n",
    "# Make PySpark importable\n",
    "import findspark\n",
    "findspark.init('/content/spark-3.5.1-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRmWX7kM7ux2"
   },
   "source": [
    "With everything set up, let's run a local session to test if the installation worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wvf3ZJ4Q-Y2i",
    "outputId": "32dc1a06-3ede-49c9-8f55-a91df448dd1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|label|\n",
      "+---+-----+\n",
      "|  1|  foo|\n",
      "|  2|  bar|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Introduction\").getOrCreate()\n",
    "\n",
    "# Test the Spark session\n",
    "df = spark.createDataFrame([(1, 'foo'), (2, 'bar')], ['id', 'label'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHcu41at9FQk"
   },
   "source": [
    "## **1 Using Spark with Pythom**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oH_0dtfo-o6u"
   },
   "source": [
    "The first step in using Spark is to connect to a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLeoW4ZK-xSo"
   },
   "source": [
    "In a practical scenario, the cluster will be hosted on a remote machine connected to all other nodes. This setup includes a primary machine known as the master, responsible for distributing data and computations. The master communicates with the other machines in the cluster, known as workers. The master delegates tasks and data to the workers for processing, and they return the results to the master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDk0U2Xs9iPk"
   },
   "source": [
    "### **1.1 Creating a SparkSession**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1k3dilG9ocG"
   },
   "source": [
    "Creating multiple `SparkSessions` and `SparkContexts` can lead to issues, so it is a best practice to use the `SparkSession.builder.getOrCreate()` method. This method returns an existing `SparkSession` if one is already present in the environment, or it creates a new one if necessary. This approach ensures that you avoid problems associated with having multiple sessions or contexts running simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Dq94ugDxACLN"
   },
   "outputs": [],
   "source": [
    "# Start a local session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Introduction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvI-yUhfAFkP",
    "outputId": "88f7f8f6-dd24-4624-8bed-6e67a6be4826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x78a9e46d4850>\n",
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "# Verify SparkContext\n",
    "print(spark)\n",
    "\n",
    "# Print Spark version\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR6s7qN9BZET"
   },
   "source": [
    "### **1.2 Using DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI-TY69TBdlc"
   },
   "source": [
    "The main data structure in Spark is the Resilient Distributed Dataset (RDD). This is a low-level object that enables Spark to perform its magic by distributing data across multiple nodes in the cluster. However, working directly with RDDs can be challenging, so in this lesson, you'll use the Spark DataFrame abstraction built on top of RDDs.\n",
    "\n",
    "The Spark DataFrame is designed to behave similarly to a SQL table (with variables in columns and observations in rows). Not only are DataFrames easier to understand, but they are also more optimized for complex operations than RDDs.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to achieve the same result, but some methods are significantly more time-consuming than others. With RDDs, it's up to the data scientist to determine the correct way to optimize the query, but the DataFrame implementation has a lot of this optimization built in!\n",
    "\n",
    "To begin working with Spark DataFrames, you first need to create a `SparkSession` object from your `SparkContext`. You can think of the `SparkContext` as your connection to the cluster and the `SparkSession` as your interface to that connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI9R5Cm1BpM0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize SparkSession with a specific application name\n",
    "spark = SparkSession.builder.appName(\"sp-functions\").getOrCreate()\n",
    "\n",
    "# Read CSV files from a specified directory, with headers and inferred schema\n",
    "df = spark.read.csv(r\"C:\\Users\\alefr\\PycharmProjects\\series-spark\\data\\*.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the dataframe\n",
    "df.show()\n",
    "\n",
    "# ---------------------- #\n",
    "# DataFrame Operations\n",
    "# ---------------------- #\n",
    "\n",
    "# Select specific columns using `select`\n",
    "df.select(\"cnpj_raiz\", \"socios\", \"atualizado_em\").show()\n",
    "\n",
    "# Select columns using `select` with column objects\n",
    "df.select(col(\"cnpj_raiz\"), col(\"socios\")).show()\n",
    "\n",
    "# Add a constant column using `lit`\n",
    "df.select(lit(1).alias(\"valid_row\")).show()\n",
    "\n",
    "# Add a calculated column using `withColumn` and a conditional expression\n",
    "df.withColumn(\"valid_row\", when(col(\"cnpj_raiz\").isNull(), lit(0)).otherwise(lit(1))).show(5)\n",
    "\n",
    "# Generate unique IDs using `monotonically_increasing_id`\n",
    "df.select(monotonically_increasing_id().alias(\"id\"), \"cnpj_raiz\").show(5)\n",
    "\n",
    "# ---------------------- #\n",
    "# Additional Examples\n",
    "# ---------------------- #\n",
    "\n",
    "# Create a new DataFrame with a schema\n",
    "df_dados = spark.createDataFrame([\n",
    "    (\"a\", 1, 3, 4),\n",
    "    (\"b\", 2, 4, 5),\n",
    "    (\"c\", 3, 5, 6)\n",
    "], schema='a string, b int, c int, d int')\n",
    "\n",
    "# Find the greatest value among columns\n",
    "df_dados.select(\n",
    "    \"b\",\n",
    "    \"c\",\n",
    "    \"d\",\n",
    "    greatest(\"b\", \"c\", \"d\").alias(\"greatest_value\")\n",
    ").show()\n",
    "\n",
    "# Use SQL-like expressions in `select`\n",
    "df.select(\"cnpj_raiz\", expr(\"CASE WHEN LENGTH(cnpj_raiz) = 8 THEN 1 ELSE 0 END\").alias(\"flag\")).show(5)\n",
    "\n",
    "# Round numerical values to a specified number of decimal places\n",
    "df_dados.select(round(\"b\", 2).alias(\"b_rounded\")).show()\n",
    "\n",
    "# Display current date and timestamp\n",
    "df.select(current_date(), current_timestamp()).show()\n",
    "\n",
    "# ---------------------- #\n",
    "# Transformations\n",
    "# ---------------------- #\n",
    "\n",
    "# Define a function to transform a DataFrame\n",
    "def make_upper(df):\n",
    "    return df.withColumn(\"a\", upper(col(\"a\")))\n",
    "\n",
    "# Apply the transformation function\n",
    "df_dados.transform(make_upper).show()\n",
    "\n",
    "# ---------------------- #\n",
    "# Aggregations\n",
    "# ---------------------- #\n",
    "\n",
    "# Calculate the average of a column\n",
    "df_dados.select(avg(\"b\")).show()\n",
    "\n",
    "# ---------------------- #\n",
    "# Filtering and Grouping\n",
    "# ---------------------- #\n",
    "\n",
    "# Filter rows based on a condition\n",
    "df.filter(df[\"cnpj_raiz\"] == \"2421421\").show()\n",
    "\n",
    "# Group by a column and count occurrences\n",
    "df_cnpj_por_data = df.groupBy(\"atualizado_em\").count()\n",
    "df_cnpj_por_data.show()\n",
    "\n",
    "# Write the grouped DataFrame as a Parquet table\n",
    "df_cnpj_por_data.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"cnpj_por_data\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO5d+gJa8KOtYgASifyVAG/",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
